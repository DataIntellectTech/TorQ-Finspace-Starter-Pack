{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorQ in Managed kdb Insights Demo Pack","text":"<p>The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes:</p> <ul> <li> <p>an example set of historic data</p> </li> <li> <p>a simulated data feed</p> </li> <li> <p>configuration changes for base TorQ</p> </li> <li> <p>start and stop scripts using Terraform</p> </li> </ul> <p>Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7.</p> <p>email: torqsupport@dataintellect.com</p> <p>web: www.dataintellect.com</p>"},{"location":"01-whatistorq/","title":"What Is TorQ?","text":"<p>Data Intellect TorQ is a comprehensive framework built on top of the kdb+ database system. It serves as the foundation for creating production-ready kdb+ systems by providing core functionality and utilities that allow developers to focus on building application business logic. TorQ incorporates numerous best practices, emphasizing performance, maintainability, supportability and extensibility.</p> <p>Key features of Data Intellect TorQ include process management, code management, configuration management, usage logging, connection management (both incoming and outgoing), access controls, timer extensions, standard output/error logging, error handling, documentation and development tools. Currently TorQ within Managed kdb Insights is a minimum viable product (MVP), meaning that not all TorQ functionality has been implemented but in future iterations more functionality will be added.</p> <p>If you would like to read more information on TorQ and all of its functionality, please follow this link. Additional background on this project is here.</p>"},{"location":"02-Introduction/","title":"Introduction","text":""},{"location":"02-Introduction/#goals-of-the-workshop","title":"Goals of the workshop","text":"<ol> <li> <p>To understand what TorQ is and to know the benefits of using it with Managed kdb Insights.</p> </li> <li> <p>To set up a TorQ stack within Managed kdb Insights, connect to the clusters, and access the data within - both historical and real-time.</p> </li> <li> <p>Show existing TorQ users how to migrate their database into Managed kdb Insights.</p> </li> </ol>"},{"location":"02-Introduction/#what-are-we-going-to-build","title":"What are we going to build?","text":"<p>We will be building \u201cTorQ for Amazon FinSpace with Managed kdb Insights\u201d, a MVP of TorQ which is leveraging functionality within AWS. In this MVP, although all the TorQ code will be included within your code bucket, we will only be using files which are a necessity for this MVP creation. This will create a working TorQ setup on the cloud through Managed kdb Insights. We are going to do so by replicating the below steps.</p> <ul> <li> <p>Creating and setting up a Kdb Environment on Amazon Finspace.</p> </li> <li> <p>Create a General Purpose (GP) cluster for the Discovery process of TorQ. This allows other processes to use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability.</p> </li> <li> <p>Create a Ticker Plant (TP) cluster. This will receive data from the feed and send it to it's subscribers (RDB in this setup). In this setup we use a Segmented Ticker Plant (STP) but for simplicity it will just be called TP for this documentation.</p> </li> <li> <p>Create an RDB cluster. This will allow us to query and store live data from the TP.</p> </li> <li> <p>Create a HDB cluster. This will allow us to query historical data.</p> </li> <li> <p>Create Gateway cluster which acts as the gateway within TorQ. This process allows users to query data within the RDB and HDB processes.</p> </li> <li> <p>Lastly, create another General Purpose (GP) cluster within Managed kdb Insights. This will replicate the feed handler of TorQ, which will normalize and prepare our data into a schema readable by kdb, for the ingestion and population of our tables.</p> </li> </ul> <p>All of this culminates in a TorQ production system being hosted on the cloud using six clusters. This allows users to ingest data, before querying both live and historical data through a gateway and discovery process.</p>"},{"location":"03-torqwithmanagedkdbinsights/","title":"TorQ with Managed kdb Insights","text":"<p>When porting TorQ to AWS we chose to make a minimum viable product and then build out features on top of this to give clients a working AWS solution that includes the essential TorQ features. We intend to align with the Managed kdb Insights roadmap, adding additional functionality to the TorQ implementation as new features become available. The processes available in our first iteration of TorQ for Amazon FinSpace with Managed kdb Insights are the following:</p> <ul> <li> <p>Discovery: Processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections - it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections.</p> </li> <li> <p>Ticker Plant: The TP recieves data, either from a feed handler or directly from a source, and published that data to it's subscribers, usually either an RDB, or another client process. TP subscribers have the option to subscribe to all data, or just a subset of data.</p> <ul> <li>Currently we use a type of TP called an STP (Segmented Ticker Plant) which allows us the option to use not only trigger EOD (End Of Day) but also EOP (End of Period) if needed.</li> </ul> </li> <li> <p>Historical Database: The HDB holds data from before the current day. It is read only and used for querying all historical data. Data is stored in date partitions and can be queried through the gateway process. Note: The HDB is unqueriable for around 5 minutes during EOD (End Of Day) processing.</p> </li> <li> <p>Real-time Database: The RDB subscribes and captures all data from the feed handler throughout the current day and store it in memory for query or real-time processing.</p> </li> <li> <p>Gateway: The gateway acts as a single interface point that separates the end user from the configuration of underlying databases. You don't need to know where data is stored, and you don't need to make multiple requests to retrieve it. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients.</p> </li> <li> <p>Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and sending it on to our TP.</p> </li> </ul> <p>These features allow us to store real-time and historical data and make it available to users.</p> <p> </p>"},{"location":"03-torqwithmanagedkdbinsights/#notable-differences-within-this-reduced-version-of-torq-in-comparison-to-normal-torq","title":"Notable Differences within this reduced version of TorQ in comparison to normal TorQ","text":""},{"location":"03-torqwithmanagedkdbinsights/#envq","title":"env.q","text":"<p>The entry point script is <code>env.q</code> which sets initial environment variables and loads the main <code>torq.q</code> script. This is used because Managed kdb Insights does not allow setting environment variables or running shell scripts at startup. The \u201cenv.q\u201d file is the first of our files loaded in and it then specifies based on start-up parameters what to load in next. It also references and connects to the setup created inside Managed kdb Insights . For example, the database name for your AWS environment is referenced inside of this file.</p>"},{"location":"03-torqwithmanagedkdbinsights/#loading-codehdbs","title":"Loading code/hdbs","text":"<p>Generally the main process code file (e.g. <code>code/processes/discovery.q</code>) is passed as a command line parameter with the <code>-load</code> flag. We can't do this in Managed kdb Insights as filepaths are not allowed as command line parameters (yet). To work around this, we include a <code>.proc.params[`load]</code> variable in the settings file. We use the same approach for loading data directories into HDBs.</p>"},{"location":"04-prerequisites/","title":"Prerequisites","text":"<ul> <li>An AWS account with an AdministratorAccess policy to create the Managed kdb resources.</li> <li>A KX insights license applied to your account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace.</li> <li>Inside a Linux system you will need to download code from the TorQ and TorQ-Amazon-FinSpace-Starter-Pack GitHub repositories - Instructions below.</li> <li>If you are NOT using our Terraform deployment option to create and set up your Kdb Environment, follow this AWS workshop to do so.</li> </ul>"},{"location":"04-prerequisites/#downloading-the-code","title":"Downloading the Code","text":""},{"location":"04-prerequisites/#torq","title":"TorQ","text":"<p>Take note of the latest version of code from the TorQ Latest Release Page - release name are v#.#.# e.g. v1.0.0</p> <p>Run the following code - ensure you replace <code>&lt;copied_version_name&gt;</code> with the release version you took note of above.</p> <pre><code>git clone --depth 1 --branch &lt;copied_version_name&gt; https://github.com/DataIntellectTech/TorQ.git\n</code></pre>"},{"location":"04-prerequisites/#torq-amazon-finspace-starter-pack","title":"TorQ Amazon FinSpace Starter Pack","text":"<p>Take note of the latest version of code from the TorQ-Amazon-FinSpace-Starter-Pack Latest Release Page - release name are v#.#.# e.g. v1.0.0</p> <p>Run the following code - ensure you replace <code>&lt;copied_version_name&gt;</code> with the release version you took note of above.</p> <pre><code>git clone --depth 1 --branch &lt;copied_version_name&gt; https://github.com/DataIntellectTech/TorQ-Amazon-FinSpace-Starter-Pack.git\n</code></pre>"},{"location":"04-prerequisites/#zip-them-up-together","title":"Zip them up together","text":"<p>Now we will zip these files together:</p> <pre><code>zip -r code.zip TorQ/ TorQ-Amazon-FinSpace-Starter-Pack/ -x \"TorQ*/.git*\"\n</code></pre>"},{"location":"04-prerequisites/#create-and-upload-code-to-s3-for-non-terraform-deployment-only","title":"Create and Upload code to S3 (For Non Terraform Deployment Only)","text":"<p>Two S3 buckets are required for this setup - one for the code and one for the data.</p> <p>Create your S3 bucket by searching for \"S3\" and clicking <code>Create bucket</code></p> <p>Choose the same AWS Region as your AWS Finspace KxEnvirnment</p> <p>Give your bucket a name</p> <p> </p> <p>Unselect the <code>Block all public access</code> box</p> <p> </p> <p>Leave all other settings as the default</p>"},{"location":"04-prerequisites/#edit-the-access-policy","title":"Edit the access policy","text":"<p>Copy the ARN of your S3 buckets in the console by navigating to your S3 bucket, selecting <code>Properties</code></p> <p> </p> <p>Edit the Access policy of both S3 buckets with the JSON document:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"FinSpaceServiceAccess\",\n    \"Statement\": [{\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"finspace.amazonaws.com\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"&lt;ARN OF BUCKET COPIED EARLIER&gt;/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceAccount\": \"766012286003\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"finspace.amazonaws.com\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"&lt;ARN OF BUCKET COPIED EARLIER&gt;\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceAccount\": \"766012286003\"\n                }\n            }\n        }\n    ]\n }\n</code></pre>"},{"location":"04-prerequisites/#upload-code-to-s3-code-bucket","title":"Upload code to S3 code bucket","text":"<p>Upload the zip file created earlier to AWS S3 codebucket:</p> <pre><code>aws s3 cp code.zip s3://&lt;your S3 codebucket name&gt;\n</code></pre>"},{"location":"04-prerequisites/#upload-hdb-to-s3-data-bucket","title":"Upload hdb to S3 data bucket","text":"<p>Copy the pre-packaged hdb to AWS S3 databucket:</p> <pre><code>aws s3 cp --recursive TorQ-Amazon-FinSpace-Starter-Pack/hdb s3://&lt;your S3 data bucket name&gt;/hdb\n</code></pre>"},{"location":"05-terraform/","title":"Terraform","text":"<p>The process of setting up a working Managed kdb environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. </p> <p>This Terraform script can be used to deploy an entire environment from scratch. This will include:</p> <ul> <li> <p>Creating and uploading data to S3 buckets with required policies</p> </li> <li> <p>Creating IAM roles</p> </li> <li> <p>Creating network and transit gateway</p> </li> <li> <p>Deploying clusters</p> </li> </ul> <p>It is split into two modules, one for the environment and one for the clusters. This makes the directory more organised and cluster deployments easier to manage. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation.</p> <p> </p> <p>This Terraform setup is designed to deploy and manage a Managed kdb Insights environment running a TorQ bundle.</p>"},{"location":"05-terraform/#prerequisites","title":"Prerequisites","text":"<ol> <li>Ensure you have followed the standard prerequisite steps to ensure you have the latest versions of code.</li> <li>Have the latest version of the AWS CLI installed.</li> <li>Have the latest version of Terraform installed.</li> <li>Configure the AWS CLI to your AWS account.</li> <li>Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions.</li> <li>Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region.</li> <li>This instruction refers to Linux and would only work under the Linux environment.</li> </ol>"},{"location":"05-terraform/#resource-link","title":"Resource Link","text":"<ul> <li>For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform.</li> </ul>"},{"location":"05-terraform/#how-to-use-initial-deployment","title":"How to Use - Initial Deployment","text":"<p>New user please continue and follow this section - Users with existing infrastructure, please skip to our existing infrastructure section.</p> <ol> <li>(Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in <code>TorQ-Amazon-FinSpace-Starter-Pack/inthdb/hdb</code> or <code>TorQ-Amazon-FinSpace-Starter-Pack/datehdb/hdb</code></li> <li>Move into the <code>TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments</code> directory; within there should be two sub-directories : <code>date-par</code> and <code>int-par</code>.    If you want your finspace with Managed Kdb Insights system to use date partitions use <code>date-par</code>   If you want your system to perform multiple intra-day writedowns with int partitions use <code>int-par</code> <code>cd</code> into one of the two sub-directories. This will be your Terraform working directory from which you should run all <code>terraform</code> commands.   (Note: All writedowns in Finspace with Managed Kdb Insights will be done through changesets)</li> <li>(Optional) symlink your hdb and your zipped source code to your working directory</li> <li>Modify variables inside the <code>terraform.tfvars</code> file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of <code>\"Name\"</code>. For example, For the variable on <code>role-name</code>, you can change the variable name by replacing <code>\"finspace-role\"</code>.</li> <li>(Optional) If you have changed the database name from the default <code>finspace-database</code> to any other names, please also edit the <code>env.q</code> file, changing the database name to the new variable that you have set in line 19.</li> <li>Run <code>aws configure</code> in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key Prerequisites.</li> <li>From your Terraform working directory which is <code>TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments/date-par|int-par</code>, run <code>terraform init</code>.</li> <li>If initialized without error, run <code>terraform plan</code>. This will show all resources set to be created or destroyed by Terraform.</li> <li>Run <code>terraform apply</code> to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in <code>nohup</code>. (Using <code>nohup</code> might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.) Example nohup run: <code>nohup terraform apply -auto-approve &gt; terraform_apply.log 2&gt;&amp;1 &amp;</code>. </li> </ol> <p>You can now skip ahead to our Managing Your Infrastructure section</p>"},{"location":"05-terraform/#how-to-use-deploying-with-terraform-for-users-with-existing-infrastructure","title":"How to Use - Deploying With Terraform For Users With Existing Infrastructure","text":"<p>For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin.</p> <p>Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle.</p> <p>Move into the <code>deployments</code> directory, and you'll see an <code>imports.tf</code> file (currently empty). This <code>imports.tf</code> file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system.</p>"},{"location":"05-terraform/#terraform-import-block-syntax","title":"Terraform Import Block Syntax","text":"<pre><code>import {\n  to = aws_instance.example\n  id = \"i-abcd1234\"\n}\n\nresource \"aws_instance\" \"example\" {\n  name = \"hashi\"\n  # (other resource arguments...)\n}\n</code></pre> <p>The above <code>import</code> block defines an import of the AWS instance with the ID \"i-abcd1234\" into the <code>aws_instance.example</code> resource in the root module.</p> <p>The import block has the following arguments:</p> Argument Description Example <code>to</code> The instance address this resource will have in your state file. <code>to = aws_instance.example</code> resourse (e.g. <code>id</code> or <code>name</code>) A string with the import information of the resource. Example 1. <code>id = \"i-abcd1234\"</code> Example 2. <code>name = \"aws/vendedlogs/finspace/myclustername\"</code> <code>provider</code> (optional) An optional custom resource provider. If you do not set the provider argument, Terraform attempts to import from the default provider. See The Resource provider Meta-Argument for details. <p>The import block's ID/name argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this detail to locate the resource you want to import.</p> <p>The import ID/name must be known at plan time for planning to succeed. If the value of <code>id</code>/<code>name</code> is only known after apply, <code>terraform plan</code> will fail with an error.</p> <p>The identifier you use for a resource's import ID/name is resource-specific. You can find the required ID/name in the provider documentation for the resource you wish to import.</p>"},{"location":"05-terraform/#terraform-import-block-template","title":"Terraform import block Template","text":"<p>We have created a Terraform import block template in <code>terraform-deployment/importtemplate.md</code>. In this template, you can select the needed import block and paste it into the <code>imports.tf</code> file within the <code>terraform-deployment/deployments/imports.tf</code> directory. Remember to change the ID to the referring ID of your existing infrastructure.</p>"},{"location":"05-terraform/#managing-your-infrastructure","title":"Managing Your Infrastructure","text":"<p>Once your environment is up and running, you can use this configuration to manage it:</p> <ol> <li>Code Updates: If you make any code changes in <code>TorQ</code> or <code>TorQ-Amazon-FinSpace-Starter-Pack</code> and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code.</li> <li>Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in <code>clusters/rdb.tf</code> and run Terraform again. The RDB will be recreated with the new node size.</li> <li>Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the <code>terraform.tfvars</code> file. To delete a cluster, set its count to 0. To delete all clusters, set <code>create-clusters</code> to 0.</li> <li>(int-par deployment only) log groups and metric filters: These resources are only created if the dependent log groups exists so by default are ignored. Once your kxenvironment is up, set <code>create-mfilters</code> flag to 'true' and update the <code>wdb_log_groups</code> variable in <code>terraform.tfvars</code> to include the log groups of your clusters you wish to monitor. Then rerun <code>terraform apply</code></li> </ol>"},{"location":"05-terraform/#basic-commands-in-terraform","title":"Basic Commands in Terraform","text":"<ul> <li><code>terraform init</code>       -   Prepare your working directory for other commands</li> <li><code>terraform validate</code>   -   Check whether the configuration is valid</li> <li><code>terraform plan</code>       -   Show changes required by the current configuration</li> <li><code>terraform apply</code>      -   Create or update infrastructure</li> <li><code>terraform destroy</code>    -   Destroy previously-created infrastructure</li> <li>For more commands in Terraform, please visit Terraform Command </li> </ul>"},{"location":"05-terraform/#terraform-state-management","title":"Terraform State Management","text":"<p>Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management:</p> <ul> <li>Always store your state files securely, as they may contain sensitive information.</li> <li>Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations.</li> <li>Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state.</li> </ul>"},{"location":"05-terraform/#list-of-aws-structures-that-will-be-created-with-our-terraform-deployment","title":"List of AWS Structures that will be created with our Terraform deployment","text":"<ul> <li>module.environment.data.aws_iam_policy_document.iam-policy</li> <li>module.environment.data.aws_iam_policy_document.s3-code-policy</li> <li>module.environment.data.aws_iam_policy_document.s3-data-policy</li> <li>module.environment.aws_ec2_transit_gateway.test</li> <li>module.environment.aws_finspace_kx_database.database</li> <li>module.environment.aws_finspace_kx_environment.environment</li> <li>module.environment.aws_finspace_kx_user.finspace-user</li> <li>module.environment.aws_iam_policy.finspace-policy</li> <li>module.environment.aws_iam_role.finspace-test-role</li> <li>module.environment.aws_iam_role_policy_attachment.policy_attachment</li> <li>module.environment.aws_s3_bucket.finspace-code-bucket</li> <li>module.environment.aws_s3_bucket.finspace-data-bucket</li> <li>module.environment.aws_s3_bucket_policy.code-policy</li> <li>module.environment.aws_s3_bucket_policy.data-policy</li> <li>module.environment.aws_s3_bucket_public_access_block.code_bucket</li> <li>module.environment.aws_s3_bucket_public_access_block.data_bucket</li> <li>module.environment.aws_s3_bucket_versioning.versioning</li> <li>module.environment.null_resource.create_changeset</li> <li>module.environment.null_resource.upload_hdb</li> <li>module.lambda.data.archive_file.lambda_my_function</li> <li>module.lambda.data.aws_iam_policy_document.assume_events_doc</li> <li>module.lambda.data.aws_iam_policy_document.assume_lambda_doc</li> <li>module.lambda.data.aws_iam_policy_document.assume_states_doc</li> <li>module.lambda.data.aws_iam_policy_document.ec2-permissions-lambda</li> <li>module.lambda.data.aws_iam_policy_document.eventBridge_policy_doc</li> <li>module.lambda.data.aws_iam_policy_document.finspace-extra</li> <li>module.lambda.data.aws_iam_policy_document.lambda_basic_execution</li> <li>module.lambda.data.aws_iam_policy_document.lambda_error_queue_access_policy_doc</li> <li>module.lambda.data.aws_iam_policy_document.lambda_invoke_scoped_access_policy_doc</li> <li>module.lambda.data.aws_iam_policy_document.sns_publish_scoped_access_policy_doc</li> <li>module.lambda.data.aws_iam_policy_document.xray_scoped_access_policy_doc</li> <li>module.lambda.aws_cloudwatch_event_rule.rotateRDB_eventRule</li> <li>module.lambda.aws_cloudwatch_event_rule.rotateWDB_eventRule</li> <li>module.lambda.aws_cloudwatch_event_target.onRotateRDB_target</li> <li>module.lambda.aws_cloudwatch_event_target.onRotateWDB_target</li> <li>module.lambda.aws_iam_policy.eventBridge_policy</li> <li>module.lambda.aws_iam_policy.lambda_basic_policy</li> <li>module.lambda.aws_iam_policy.lambda_ec2_policy</li> <li>module.lambda.aws_iam_policy.lambda_finspace_policy</li> <li>module.lambda.aws_iam_policy.lambda_invoke_scoped_access_policy</li> <li>module.lambda.aws_iam_policy.sns_publish_scoped_access_policy</li> <li>module.lambda.aws_iam_policy.xray_scoped_access_policy</li> <li>module.lambda.aws_iam_role.eventBridge_role</li> <li>module.lambda.aws_iam_role.lambda_errorFormat_execution_role</li> <li>module.lambda.aws_iam_role.lambda_execution_role</li> <li>module.lambda.aws_iam_role.lambda_onConflict_execution_role</li> <li>module.lambda.aws_iam_role.states_execution_role</li> <li>module.lambda.aws_iam_role_policy_attachment.attach1</li> <li>module.lambda.aws_iam_role_policy_attachment.attach2</li> <li>module.lambda.aws_iam_role_policy_attachment.attach3</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_basic_to_errorFormat</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_basic_to_onConflict</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_ec2_policy_to_onConflict</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_eventBridge_policy</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_finspace_policy_to_onConflict</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_lambda_invoke_scoped_access_policy</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_sns_publish_scoped_access_policy</li> <li>module.lambda.aws_iam_role_policy_attachment.attach_xray_scoped_access_policy</li> <li>module.lambda.aws_lambda_function.finSpace-rdb-errorFormat-lambda</li> <li>module.lambda.aws_lambda_function.finSpace-rdb-lambda</li> <li>module.lambda.aws_lambda_function.finSpace-rdb-onConflict-lambda</li> <li>module.lambda.aws_sfn_state_machine.sfn_state_machine</li> <li>module.lambda.aws_sns_topic.lambda_error_topic</li> <li>module.lambda.aws_sns_topic_subscription.lambda_error_email_target[0]</li> <li>module.lambda.aws_sns_topic_subscription.lambda_error_queue_target</li> <li>module.lambda.aws_sqs_queue.lambda_error_queue</li> <li>module.lambda.aws_sqs_queue_policy.lambda_error_queue_access_policy</li> <li>module.lambda.local_file.lambda_configs</li> <li>module.metricfilter.data.aws_cloudwatch_log_group.wdb_log_groups[\"*\"]</li> <li>module.metricfilter.aws_cloudwatch_event_rule.wdb_log_monit_rule[0]</li> <li>module.metricfilter.aws_cloudwatch_event_target.wdb_log_monit_rule_target[0]</li> <li>module.metricfilter.aws_cloudwatch_log_metric_filter.wdb_log_monit[\"*\"]</li> <li>module.metricfilter.aws_cloudwatch_metric_alarm.wdb_log_monit_alarm[0]</li> <li>module.network.aws_internet_gateway.finspace-igw</li> <li>module.network.aws_route.finspace-route</li> <li>module.network.aws_route_table.finspace-route-table</li> <li>module.network.aws_security_group.finspace-security-group</li> <li>module.network.aws_route_table_association.subnet-assocations[0]</li> <li>module.network.aws_route_table_association.subnet-assocations[1]</li> <li>module.network.aws_route_table_association.subnet-assocations[2]</li> <li>module.network.aws_subnet.finspace-subnets[0]</li> <li>module.network.aws_subnet.finspace-subnets[1]</li> <li>module.network.aws_subnet.finspace-subnets[2]</li> <li>module.network.aws_subnet.finspace-subnets[3]</li> <li>module.network.aws_vpc.finspace-vpc</li> </ul>"},{"location":"05-terraform/#destorying-your-infastructure","title":"Destorying your infastructure","text":"<p>Normally, you should be able to take down your entire stack by running <code>terraform destroy</code>. There are some known limitations in this terraform stack, and a few manual steps are involved: 1. Delete any clusters that have been created manually or by any lambdas. On the console navigate to your kx environment, select the 'Clusters' tab, and delete each cluster by selecting the cluster and clicking 'Delete'. 2. manually delete your hdb. This is because the hdb files are deployed using \"local-exec\" but not managed by terraform istelf. To do this run : <code>aws s3 rm --region \"&lt;region&gt;\" --recursive s3://&lt;your bucket name&gt;/hdb/</code>. 3. Run <code>terraform destroy</code> 4. Sometimes terraform does not report if the transit gateway has been destroyed. Navigate to VPC &gt; Transit Gateways on the AWS console, and delete any outstanding transit gateways associated with your kx environment (you may need to delete any transit gateway attachments before deleting the transit gateway itself)</p>"},{"location":"05-terraform/#references-and-documentation","title":"References and Documentation","text":"<p>For more in-depth information and documentation, explore the following resources:</p> <ul> <li>Terraform Documentation</li> <li>AWS Documentation</li> </ul> <p>These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.</p>"},{"location":"06-scalinggroupandvolumecreation/","title":"Creating Kx Managed Insight Scaling Groups, Shared Volumes, and AWS Finspace Dataviews","text":"<p>If you have set up your environment using our Terraform deployment option, this page is purely informative. These resources will have be created for you by Terraform.</p>"},{"location":"06-scalinggroupandvolumecreation/#scaling-groups","title":"Scaling Groups","text":"<p>To create a scaling group through the AWS Console, select your kdb environment and navigate to the <code>Kdb scaling groups</code> tab:</p> <p> </p> <p> </p> <p>Click the <code>Create kdb scaling group</code> button</p> <ol> <li>Provide a <code>Name</code> for the kdb scaling group unique to the kdb environment. You will be asked to choose a <code>Host type</code> as well. Choose \"kx.sg.4xlarge\" or larger host type.</li> <li>Select an Availability Zone make sure it includes your previous created subnet</li> <li>Click the <code>Create kdb scaling group</code> button when you are happy with the settings.</li> </ol> <p> </p>"},{"location":"06-scalinggroupandvolumecreation/#shared-volume","title":"Shared Volume","text":"<p>To create a shared volume through the AWS Console, select your kdb environment and navigate to the <code>Volumes</code> tab:</p> <p> </p> <p>Click the <code>Create volume</code> button</p> <ol> <li>Provide a <code>Name</code> for the volume unique to the kdb environment. You will be asked to choose a <code>Volume type</code>. For now, \"NAS_1\" is the only option</li> <li>Under NAS_1 configurations, provide details for the hardware type and the amount of disk capacity allocated. <ul> <li>Choose either SSD_250 or SSD_1000 for the best performance</li> <li>The size of allocated disk space must be at least 1200 GiB.</li> </ul> </li> </ol> <p> </p> <ol> <li>Choose an Availability Zone. It is recommended that it matches with the Availability Zone you assigned the kdb scaling group to run on</li> <li>Click the <code>Create volume</code> button when you are happy with the settings</li> </ol> <p> </p>"},{"location":"06-scalinggroupandvolumecreation/#optional-dataview","title":"(Optional) Dataview","text":"<p>You must have a shared volume created to perform this step</p> <p>If you plan to run your HDB cluster on a scaling group this step is required. Otherwise, this step is optional.</p> <p>To create a scaling group through the AWS Console, select your kdb environment and navigate to the <code>Databases</code> tab:</p> <p> </p> <p>Select the database that has the changesets appropriate for your use case. To learn more about changesets click this link</p> <ol> <li>Navigate to the <code>Dataview</code> tab and click the <code>Create dataview</code> button</li> </ol> <p> </p> <ol> <li>Under <code>Dataview details</code> provide a <code>Name</code> for your dataview that is unique to your kdb environment</li> <li>Choose an Availability Zone. This must match the Availability Zone your kdb scaling group runs on</li> </ol> <p> </p> <ol> <li>Under <code>Changeset update settings</code> you have the option of choosing two modes:<ul> <li>Auto-update : (Recommended) The dataview will automatically use data from the latest changeset</li> <li>Static      : The dataview will use data from a pre-determined changeset id</li> </ul> </li> <li>Under <code>Segment configuration - optional</code> choose the root path for your <code>Database path</code> and the <code>Volume</code> you created in the prior step</li> </ol> <p> </p> <p>Click <code>Create dataview</code> when you are happy with the settings</p>"},{"location":"07-clustercreation/","title":"Creating TorQ Clusters","text":"<p>If you have set up your environment using our Terraform deployment option, this page is purely informative. Your clusters will have been created for you by Terraform.</p> <p>To create a cluster, first select your kdb environment:</p> <p> </p> <p>Then select the <code>Clusters</code> tab, then either of the <code>Create cluster</code> buttons:</p> <p> </p>"},{"location":"07-clustercreation/#prerequisites","title":"Prerequisites","text":"<p>For these clusters, you will require:</p> <ul> <li>A kdb scaling group</li> <li>A database with:</li> <li>A changeset</li> <li>A dataview</li> <li>A volume</li> </ul>"},{"location":"07-clustercreation/#discovery-cluster","title":"Discovery Cluster","text":"<ol> <li> <p>Set the <code>Cluster type</code> to <code>General purpose</code>, also known as \"GP\".</p> </li> <li> <p>Choose a name for your cluster.</p> <ul> <li>Note: This name must match your process name (<code>procname</code>) added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but <code>procname</code> is. Our suggestion is to use the process type (<code>proctype</code>) with a number, e.g. <code>discovery1</code>.</li> </ul> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> <p><p> </p></p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>In the <code>Node details</code> section, set the <code>Memory reservation per node</code> to the minimum allowed (6 MiB) and leave the rest blank.</p> <p><p> </p></p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> <p><p> </p></p> <p><p> </p></p> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype discovery procname discovery1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Leave everything as blank and click <code>Next</code> to move on to the next page.</p> <p><p> </p> 14. Check the entered information in the review page, then select <code>Create cluster</code>.</p> </li> </ol>"},{"location":"07-clustercreation/#ticker-plant-tp-cluster","title":"Ticker Plant (TP) Cluster","text":"<ol> <li> <p>Set the cluster type to \"Tickerplant\".</p> </li> <li> <p>Choose a name for your cluster.</p> <ul> <li>Note: This name must match your process name, <code>procname</code>, added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type (<code>proctype</code>) with a number e.g. <code>tp1</code>.</li> </ul> <p><p> </p></p> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed.</p> </li> <li> <p>Set <code>Memory reservation per node</code> to the minimum value (6 MiB).</p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype segmentedtickerplant procname tp1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your volume, then select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Check the entered information in the review page, then select <code>Create cluster</code>.</p> </li> </ol>"},{"location":"07-clustercreation/#real-time-database-rdb-cluster","title":"Real-Time Database (RDB) Cluster","text":"<ol> <li> <p>Set the cluster type to \u201cRDB\".</p> </li> <li> <p>Choose a name for your cluster.</p> <ul> <li>Note: This name must match your process name, <code>procname</code>, added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type (<code>proctype</code>) with a number e.g. <code>rdb1</code>.</li> </ul> <p><p> </p></p> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed.</p> </li> <li> <p>Set <code>Memory reservation per node</code> to the minimum value (6 MiB).</p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype rdb procname rdb1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> </li> <li> <p>In the <code>Savedown database configuration</code> section, select your database in the <code>Database name</code> dropdown menu.</p> </li> <li> <p>In the <code>Savedown storage configuration - optional</code> section, select your volume in the <code>Volume name - optional</code> dropdown menu.</p> </li> <li> <p>In the <code>Tickerplant log configuration - optional</code> section, select your volume here too from the dropdown menu.</p> <ul> <li>This will allow your RDB access to the TP logs for recovery.</li> </ul> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Check the entered information in the review page, then select <code>Create cluster</code>.</p> </li> </ol>"},{"location":"07-clustercreation/#historical-database-hdb-cluster","title":"Historical Database (HDB) Cluster","text":"<ol> <li> <p>Set the <code>Cluster type</code> to <code>HDB</code>.</p> </li> <li> <p>Choose a name for your cluster.</p> <ul> <li>Note: This name must match your process name, <code>procname</code>, added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type (<code>proctype</code>) with a number e.g. <code>hdb1</code>.</li> </ul> <p><p> </p></p> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed.</p> </li> <li> <p>Set <code>Memory reservation per node</code> to the minimum value (6 MiB).</p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype hdb procname hdb1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> </li> <li> <p>For <code>Database name</code>, select your database from the dropdown menu.</p> </li> <li> <p>For <code>Dataview name</code>, select your dataview from the dropdown menu.</p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> </ol> <p> </p> <ol> <li>Check the entered information in the review page, then select <code>Create cluster</code>.</li> </ol>"},{"location":"07-clustercreation/#gateway-cluster","title":"Gateway Cluster","text":"<p>Ensure that the Discovery cluster is in a \"Running\" state before creating the Gateway cluster.</p> <ol> <li> <p>Set the <code>Cluster type</code> to <code>Gateway</code>.</p> </li> <li> <p>Choose a name for your cluster.</p> <ul> <li>Note: This name must match your process name, <code>procname</code>, added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type (<code>proctype</code>) with a number e.g. <code>gateway1</code>.</li> </ul> <p><p> </p></p> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed.</p> </li> <li> <p>Set <code>Memory reservation per node</code> to the minimum value (6 MiB).</p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> <p><p> </p></p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype gateway procname gateway1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> </li> <li> <p>This page had no editing options. Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Check the entered information in the review page, then select <code>Create cluster</code>.</p> </li> </ol>"},{"location":"07-clustercreation/#feed-cluster","title":"Feed Cluster","text":"<p>Ensure that the RDB cluster is in a <code>Running</code> state before creating the Feed cluster.</p> <ol> <li> <p>Set the <code>Cluster type</code> to <code>General purpose</code>, also known as \"GP\".</p> </li> <li> <p>Choose a name for your cluster. As this is a sample feed and not a \"production\" intended process, please name it <code>feed1</code>.</p> <p><p> </p></p> </li> <li> <p>Select the execution role for the IAM user previously created. The user for all 6 clusters should be the same. This is so that each cluster has the correct permissions.</p> </li> <li> <p>Select <code>Run on kdb scaling group</code> for the Cluster running option.</p> <p><p> </p></p> </li> <li> <p>Choose your group in the dropdown in the <code>Kdb scaling group details</code> section.</p> <p><p> </p></p> </li> <li> <p>In the <code>Node details</code> section, set the <code>Memory reservation per node</code> to the minimum allowed (6 MiB) and leave the rest blank.</p> <p><p> </p></p> </li> <li> <p>Leave Tags as empty and select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select <code>Browse S3</code>, search and select your codebucket and select your code.zip file.</p> <ul> <li>Alternatively, you can copy the URL from the codebucket itself.</li> </ul> </li> <li> <p>Enter <code>TorQ-Amazon-FinSpace-Starter-Pack/env.q</code> as your initialization script.</p> </li> <li> <p>Select <code>Add command-line argument</code> twice and enter the keys and values in the below table:</p> Key Value proctype tradeFeed procname tradeFeed1 <p>This specified initialization script and the command line arguments will set up the necessary environment for your cluster.</p> <p><p> </p></p> </li> <li> <p>Select <code>Next</code> to go to the next page.</p> </li> <li> <p>Select your previously created VPC ID, Subnets, and Security Groups (we can use the readily available default), then select <code>Next</code> to go to the next page.</p> </li> <li> <p>Leave everything as blank and click <code>Next</code> to move on to the next page.</p> <p><p> </p></p> </li> <li> <p>Check the entered information in the review page, then select <code>Create cluster</code>.</p> </li> </ol>"},{"location":"07-clustercreation/#on-completion","title":"On Completion","text":"<p>When all clusters are up it should look like this:</p> <p> </p>"},{"location":"07-clustercreation/#errors-in-cluster-creation","title":"Errors in cluster creation","text":"<p>On cluster creation, most errors will result in your cluster going to a <code>Create failed</code> state.</p> <p> </p> <p>If that is the case you should:</p> <ul> <li> <p>Click the cluster name in the <code>Clusters</code> section of your environment.</p> <p><p> </p></p> </li> <li> <p>Scroll down the page and open the <code>Logs</code> tab. This should have a message with a more individualised error you can check.</p> </li> <li> <p>If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window.</p> <p><p> </p></p> </li> </ul> <p>It is worthwhile checking the logs even for clusters that have been created and searching for terms like <code>err</code>, <code>error</code> or <code>fail</code>.</p>"},{"location":"08-createec2/","title":"Creating and Connect to an EC2 Instance","text":""},{"location":"08-createec2/#create-a-windows-ec2-instance","title":"Create a Windows EC2 Instance","text":"<p>Navigate to the EC2 service.</p> <p> </p> <p>Select <code>Launch instance</code> to create a new EC2 instance.</p> <p> </p> <p>Most options here can be left as their defaults. Here are the ones that need selected/changing:</p> <ol> <li> <p>Select \"Windows\" from the <code>Quick Start</code> options.</p> <p><p> </p></p> </li> <li> <p>We need to create a new key pair: Select <code>Create new key pair</code>.</p> <p><p> </p></p> </li> <li> <p>Enter a name for your key pair, leave the key pair type as <code>RSA</code> and the file format as <code>.pem</code>. This will download a key file to you PC which you will use to connect to the instance.</p> <p><p> </p></p> </li> </ol> <p>The network should be in the same VPC as your cluster. Select <code>Create security group</code> that allows Remote Desktop Protocol (RDP) connections from anywhere.     - This is only for the purposes of the MVP. For customising see this page on security groups.</p> <p> </p>"},{"location":"08-createec2/#adding-your-new-security-group-to-you-ec2","title":"Adding your new security group to you EC2","text":"<p>Now we need to add the security group of your cluster to your EC2.</p> <p>Navigate to EC2 service.</p> <p> </p> <p>Select <code>Instances (running)</code>.</p> <p> </p> <p>Open your EC2 Instance.</p> <p> </p> <p>Select <code>Actions</code> -&gt; <code>Security</code> -&gt; <code>Change security groups</code>.</p> <p> </p> <p>Search and select the security group that is on your clusters, select <code>Add security group</code> then <code>Save</code>.</p> <p> </p> <p>You should now have two security groups, one from the launch wizard, and the one you added manually that is also attached to your clusters.</p> <p> </p>"},{"location":"08-createec2/#connecting-to-your-ec2-instance","title":"Connecting to your EC2 Instance","text":"<p>Open your EC2 Instance.</p> <p> </p> <p>Select <code>Connect</code>.</p> <p> </p>"},{"location":"08-createec2/#get-your-password","title":"Get your password","text":"<p>This only needs to be done once. Once you have this password you can skip this step.</p> <p>Select <code>Get password</code>.</p> <p> </p> <p>Upload the <code>.pem</code> that was saved to you PC earlier (alternativly you can just paste the contents of this file in the text box).</p> <p> </p> <p>This will return the value of your password. Keep a note of this password as you will need it to connect your EC2.</p>"},{"location":"08-createec2/#connect","title":"Connect","text":"<p>Download the remote desktop file.</p> <p> </p> <p>Run this file and enter the password you recieved above when promted. You should now be connected to the Windows remote desktop.</p>"},{"location":"09-clusterconnectionstring/","title":"Cluster Connection String","text":"<p>Terraform users can skip to our generate connection string section as Terraform will have created and set up your role and user for you.</p>"},{"location":"09-clusterconnectionstring/#ensure-your-role-has-correct-permissions-if-manually-set-up-not-useing-terraform","title":"Ensure Your Role has Correct Permissions (If manually set up (not useing Terraform))","text":""},{"location":"09-clusterconnectionstring/#policy","title":"Policy","text":"<p>The policy you created needs to have at least these permissions (Note the ARN should match that of your created kxEnvironment):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": \"finspace:ConnectKxCluster\",\n                \"Resource\": \"&lt;ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE&gt;/kxCluster/*\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": \"finspace:GetKxConnectionString\",\n                \"Resource\": \"&lt;ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE&gt;/kxCluster/*\"\n            }\n    ]\n}\n</code></pre> <p>To enable endofday savedowns and your clusters access to your S3 code and data buckets, you will also need the following statements to your policy</p> <pre><code>{\n         \"Statement\": [\n                 {\n                         \"Effect\": \"Allow\",\n                         \"Action\": [\n                             \"finspace:UpdateKxClusterDatabases\"\n                          ]\n                         \"Resource\": [\n                              \"&lt;ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE&gt;/kxCluster/*\",\n                              \"&lt;ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE&gt;/kxDatabase/*/kxDataview/*\",\n                          ]\n                 },\n                 {\n                         \"Effect\": \"Allow\",\n                         \"Action\": [\n                             \"s3:ListBucket\",\n                             \"s3:GetObject\",\n                             \"s3:GetObjectVersion\",\n                             \"s3:GetObjectTagging\"\n                          ]\n                         \"Resource\": [\n                              \"&lt;YOUR_S3_CODE_BUCKET_ARN&gt;\",\n                              \"&lt;YOUR_S3_CODE_BUCKET_ARN&gt;/*\",\n                              \"&lt;YOUR_S3_DATA_BUCKET_ARN&gt;\",\n                              \"&lt;YOUR_S3_DATA_BUCKET_ARN&gt;/*\"\n                          ] \n                 }\n         ]\n }\n</code></pre> <p>To find out how to get the ARN of your S3 buckets reference https://dataintellecttech.github.io/TorQ-Finance-Starter-Pack/04-prerequisites.md</p>"},{"location":"09-clusterconnectionstring/#role","title":"Role","text":"<p>We need to check the Trust Policy of your created role.</p> <p>Search for the role and open it in the IAM console. Go to <code>Trust relationships</code>.</p> <p>Your Trust relationship should have at least these:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"finspace.amazonaws.com\",\n                \"AWS\": \"arn:aws:iam::&lt;ACCOUNT_ID&gt;:root\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre>"},{"location":"09-clusterconnectionstring/#create-a-user-if-manually-set-up-not-useing-terraform","title":"Create a user (If manually set up (not useing Terraform))","text":"<p>In your kdb environment, go to the <code>Users</code> tab and select <code>Add user</code>.</p> <p> </p> <p>Give it a name and select the <code>IAM role</code> you created above.</p> <p> </p>"},{"location":"09-clusterconnectionstring/#generate-connection-string","title":"Generate Connection String","text":"<p>On the <code>Users</code> tab, copy the links for <code>IAM role</code> and <code>User ARN</code> for the user.</p> <p> </p> <p>Navigate to <code>CloudShell</code>.</p> <p> </p> <p>Replace <code>&lt;ARN_COPIED_FROM_ABOVE&gt;</code> with the <code>IAM Role</code> copied above and run the following (this will not return anything):</p> <pre><code>export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\\n$(aws sts assume-role \\\n--role-arn &lt;ARN_COPIED_FROM_ABOVE&gt; \\\n--role-session-name \"connect-to-finTorq\" \\\n--query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\\n--output text))\n</code></pre> <p>This lets you assume the role that you have just created by taking the values returned from the <code>aws sts assume-role</code> command and setting them in your <code>AWS_ACCESS_KEY_ID</code>, e.t.c. environment variables. NOTE - if you need to switch back to your own user within the CloudShell, you will need to run <code>unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN</code> to unset these environment variables.</p> <p>Copy your kdb Environment ID:</p> <p> </p> <p>Replace <code>&lt;YOUR_KDB_ENVIRONMENT_ID&gt;</code> with your kdb environment ID, <code>&lt;USER_ARN_COPIED_ABOVE&gt;</code> with the <code>User ARN</code>, and <code>&lt;NAME_OF_CLUSTER&gt;</code> with the name of the cluster you want to connect to. Run the following:</p> <pre><code>aws finspace get-kx-connection-string --environment-id &lt;YOUR_KDB_ENVIRONMENT_ID&gt; --user-arn &lt;USER_ARN_COPIED_ABOVE&gt; --cluster-name &lt;NAME_OF_CLUSTER&gt;\n</code></pre> <p>This will return a large connection string which can be used to connect to your cluster.</p> <p> </p>"},{"location":"10-setupqStudio/","title":"Setup qStudio","text":"<p>You must follow these steps from within your EC2 instance.</p>"},{"location":"10-setupqStudio/#download-qstudio","title":"Download qStudio","text":"<p>Navigate to the TimeStored website and download the relavent version (usually x64).</p> <p> </p> <p>Download the Microsoft C++ 2010 service pack (there is a specific DLL from this that you need that is usually installed on Windows machines).</p> <p> </p> <p>Select the relevant version (usually x64).**</p> <p> </p> <p>Run the file that was just downloaded.</p> <p>Search for \u2018Edit Environment variables\u2019 and add <code>SSL_VERIFY_SERVER=NO</code> as one of them.</p> <p> </p> <p> </p> <p> </p>"},{"location":"11-connectingusingqStudio/","title":"Connecting Using qStudio","text":"<p>Open qStudio, right-click on <code>Servers</code> in the top left corner and select <code>Add Server</code>.</p> <p> </p> <p>You will then see a pop-up showing the server properties. This is where you will add the information from the connection string (details below in the next step).</p> <p> </p> <p>Follow the step in our Generate Connection String section to get a fresh connection string. The generated connection string provides you with everything you need to fill the boxes above. See the below connection string for an example:</p> <p> </p> <p>The required fields in the Server Properties screen are contained within the connection string and are colon separated. Here,</p> <p><code>Host:</code> vpce-063d07d253dde398d-19p5nh4w.vpce-svc-0b408455b16d79292.us-west-2.vpce.amazonaws.com</p> <p><code>Port:</code> 443</p> <p><code>Name:</code> This is just the name of your process, can be anything i.e. rdb</p> <p><code>Username:</code> david-finspace</p> <p><code>Password:</code> Host=vpce-063d07d253dde398d-19p5nh4w.vpce-svc-0b408455b16d79292.us-west-2.vpce.amazonaws.com&amp;Port=443&amp;User=david-finspace&amp;Action=finspace%3AConnectKxCluster&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDUaCXVzLXdlc3QtMiJGMEQCIFOmCCIAE5hyh0obtgV8TeS9jCglbxFPEossH5f4vRhrAiAIaxSHF%2FQhjqACSlmwlLTNJBvx658bQwyiH9LVz20ehSr4AgguEAIaDDc2NjAxMjI4NjAwMyIMl3TMLJjQytFtFLaiKtUCbJrjypNOC5U8GwttcBfln2SV%2FJclct%2FmeDtnUCUcVB3bzcCUOpGQFXUR%2FtonWVO0ZO1SUu%2Bna%2Bs0l0UfMu7K7qDOQhszg1%2Fxdblx0TRi70GVunATeJlpppyQeZNUk9RHSOvGPCN8VKGBJmisbeo3bJb7MvVLak1lgVdLn%2FxUIS0uTfA62PbzU6LPE1nj85bupgLrvAMlSwvZMy3pVXJaN%2BBvk0mulmCvIYfqlqL%2FcjvWbOMQdyXsdAYCO4TRpA7I38Jmfr9h1oee2JVvVMCu41yRQhbAqtqgvNwrhgSyOQASAJSVGiIPHiXyGz5KefntOfq1zADleUrUg0Nvh6EPFj%2FavNsHZyBSapvUVVsx%2FA9aI2aHeg5P%2FtCifq%2Bnxg5vyKSX4GcB7P40pKs3Ymeb3yAUllN2tJ5VXlx7SI3p8jfTB8k72BeOzu%2BLbWs5gtb%2BlZG1cSQwzKSHrwY6wAEpzwyEjjuh50PM6OZgRBNgBEOiqLaTHqjBv7VHcjsF5FnpJqZ2AhBtQXXkFiItA%2FSZ7B0aGbL0sYJ8v7WZZIMtAvg4e0ve9VBYXf4hnl5i82T5EJfAy727E9e25Wwb%2FILZAeOjdTY0IhcgvWlLCeBJNEHEMO8h8yupkLfFQHlNvF14wrH1U8bSd934M8k%2F22%2FDTrZFlVQXe%2FeLYt%2FxOBEHfHWiMH51v2IDv39%2B%2B7%2FGq3XmrcNymPQQ0HcoJEK%2BTjg%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240301T130412Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=900&amp;X-Amz-Credential=ASIA3EWPD4QZXLHMX4GY%2F20240301%2Fus-west-2%2Ffinspace-apricot%2Faws4_request&amp;X-Amz-Signature=c134f0dc1f0cba738c0bf8547c9ca8702593ea9e9b89ee2d17892fd379524d30</p> <p> </p>"},{"location":"12-runningqueries/","title":"Running Queries","text":"<p>Some example queries have been implemented on the RDB and HDB processes. These are defined in <code>$KDBCODE/rdb/examplequeries.q</code> and <code>$KDBCODE/hdb/examplequeries.q</code>. These can be run directly on the processes themselves, or from the gateway which will join the results if querying across processes. To test, connect to the gateway cluster.</p> <p>For queries to be ran through the gateway, we must update some preferences on qStudio. Go to <code>Settings</code> -&gt; <code>Preferences</code> -&gt; <code>Connections</code> tab.</p> <p>We will need to uncheck the box labelled <code>Wrap query sent to server</code>.</p> <p> </p> <p>The queries below should then all run successfully!</p> <p> </p> <p>Example queries are listed below:</p> <pre><code>// From the gateway, run a query on the RDB\n.gw.syncexec[\"select sum size by sym from trades\";`rdb]\n\n// Run a query on the HDB\n.gw.syncexec[\"select count i by date from trades\";`hdb]\n\n// Run a freeform time bucketed query and join the results across the RDB and HDB\n// Note that this is generally bad practice as the HDB query doesn't contain a date clause\n.gw.syncexec[\"select sum size, max price by 0D00:05 xbar time from trades where sym=`IBM\";`hdb`rdb]\n\n// Run a query across the RDB and HDB which uses a different join function to add the data from both \n.gw.syncexecj[\"select sum size by sym from trades\";`rdb`hdb;sum]\n\n// Run the pre-defined functions - these are implemented to query the RDB and HDB as efficiently as possible\n\n// Run a bucketed HLOC query, both as a string and in functional form\n.gw.syncexec[\"hloc[2015.01.07;.z.d;0D12]\";`hdb`rdb]\n.gw.syncexec[(`hloc;2015.01.07;.z.d;0D12);`hdb`rdb]\n\n// Run a count by sym across a date range, and add the results. \n// Run both as a string and in functional from\n.gw.syncexecj[\"countbysym[2015.01.07;.z.d]\";`hdb`rdb;sum]\n.gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;sum]\n\n// Run a gateway query with a bespoke join function to line up results and compare today's data with historic data\n.gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;{(`sym xkey select sym,histavgsize:size%tradecount from x 0) lj `sym xkey select sym,todayavgsize:size%tradecount from x 1}]\n\n// Send a query for a process type which doesn't exist\n.gw.syncexec[\"select count i by date from trades\";`hdb`rubbish]\n\n// Send a query which fails\n.gw.syncexec[\"1+`a\";`hdb]\n</code></pre>"},{"location":"13-healthcheck/","title":"Check your system is healthy","text":""},{"location":"13-healthcheck/#check-if-the-system-is-running","title":"Check If the System Is Running","text":"<p>Below is an example of what running clusters look like. You can find this page by going to the AWS Console -&gt; <code>Amazon FinSpace</code> -&gt; <code>Kdb environments</code> -&gt; Select your environment -&gt; <code>Clusters</code></p> <p> </p>"},{"location":"13-healthcheck/#run-some-queries","title":"Run some queries","text":"<p>Follow steps on our Running Queries page to run some queries on your Gateway to check there are no errors and data is being returned (if there is data available).</p>"},{"location":"13-healthcheck/#check-tables-are-populatedgrowing","title":"Check tables are populated/growing","text":"<p>If you would like to go into each process to check their health, here are a few details on how.</p>"},{"location":"13-healthcheck/#check-server-connections-serversservers","title":"Check Server Connections - .servers.SERVERS","text":"<p>Each TorQ process has a table called <code>.servers.SERVERS</code> that is used to track connections. Each process (with the exception of the Feed) should have at least one connection in this table to the discovery process (the discovery will have connections to all live processes).</p> <p>If a handle is shown within this table - column <code>w</code> - you can assume the connection is active.</p>"},{"location":"13-healthcheck/#heartbeats-hbhb","title":"Heartbeats - .hb.hb","text":"<p>All processes regularly publish a signal to show it is still active. This signal is called a heartbeat. Heartbeats are received and a summary can be seen stored within the <code>.hb.hb</code> table.</p> <p>The discovery process can be used to view the heartbeat of all processes (with the exception of the Feed).</p>"},{"location":"13-healthcheck/#cloudwatch-alarms","title":"Cloudwatch Alarms","text":"<p>In cloudwatch, it may be beneficial to set up an alarm to alert you if a particular log has been recorded.</p> <p>The steps to do this can be found here: AWS Cloudwatch Alarms.</p> <p>You can set an alarm for any possible log output, one that may be helpful in particular is to monitor the logs for an unsuccessful writedown/creation of a changeset. At which point this may need to be investigated further.i</p>"},{"location":"14-takingitdown/","title":"Taking it Down","text":""},{"location":"14-takingitdown/#deleting-clusters","title":"Deleting clusters","text":"<p>From your Kdb environment select the cluster you want to delete and select <code>Delete</code>.</p> <p> </p> <p>On the confirmation dialog box, enter confirm then select <code>Delete</code>.</p> <p> </p>"},{"location":"14-takingitdown/#deleting-your-dataview","title":"Deleting your dataview","text":"<p>This step only applies if you created a dataview</p> <ol> <li>Select your kdb environment and navigate to the <code>Databases</code> tab. Then select the database your dataview is associated with:</li> </ol> <p> </p> <ol> <li>Navigate to the <code>Dataviews</code> tab. Select the circular button to the left of the dataview you want to delete. </li> <li>Click the <code>Delete</code> button</li> </ol> <p> </p> <ol> <li>On the confirmation dialog box, type \"confirm\" and then click the <code>Delete</code> button.</li> </ol> <p> </p>"},{"location":"14-takingitdown/#deleting-your-shared-volume","title":"Deleting your Shared Volume","text":"<ol> <li>Select your kdb environment and navigate to the <code>Volumes</code> tab. Then select the volume you like to delete and click <code>Delete</code></li> </ol> <ol> <li>On the confirmation dialog box, type \"confirm\" and then click the <code>Delete</code> button.</li> </ol>"},{"location":"14-takingitdown/#deleting-your-kdb-scaling-group","title":"Deleting your Kdb Scaling Group","text":"<p>Important: Delete any clusters running on your scaling group before deleting the scaling group</p> <ol> <li>Select your kdb environment and navigate to the <code>Kdb scaling groups</code> tab. Then select the scaling group you like to delete and click <code>Delete</code></li> </ol> <p> </p> <ol> <li>On the confirmation dialog box, type \"confirm\" and then click the <code>Delete</code> button.</li> </ol> <p> </p>"},{"location":"14-takingitdown/#deleting-your-database","title":"Deleting your database","text":"<p>From your Kdb environment select the <code>Databases</code> tab, select the database you want to delete and select <code>Delete</code>.</p> <p> </p> <p>On the confirmation dialog box, enter confirm then select <code>Delete</code>.</p> <p> </p>"},{"location":"15-conclusion/","title":"Conclusion","text":"<p>Contact us for Further Assistance</p> <p>We hope you found this AWS workshop informative and valuable. If you have any questions or require further assistance on any of the topics covered during this workshop or any related TorQ services, please don't hesitate to reach out to us at Data Intellect.</p> <p>email: torqsupport@dataintellect.com</p> <p>web: www.dataintellect.com</p>"}]}